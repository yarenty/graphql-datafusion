//! Agent types and structures

use async_graphql::SimpleObject;
use serde::{Deserialize, Serialize};

/// Insight generated by AI agents
#[derive(Debug, Clone, Serialize, Deserialize, SimpleObject)]
pub struct Insight {
    pub title: String,
    pub description: String,
    pub value: Option<f64>,
    pub tags: Vec<String>,
    pub confidence: Option<f64>,
}

/// Agent configuration
#[derive(Debug, Clone, Serialize, Deserialize, SimpleObject, async_graphql::InputObject)]
pub struct AgentConfig {
    pub agent_type: String,
    pub model: String,
    pub temperature: Option<f64>,
    pub max_tokens: Option<u32>,
}

/// Agent status information
#[derive(Debug, Clone, Serialize, Deserialize, SimpleObject)]
pub struct AgentStatus {
    pub agent_type: String,
    pub status: String,
    pub last_update: String,
    pub model: String,
    pub requests_processed: u64,
}

/// Time series data point
#[derive(Debug, Clone, Serialize, Deserialize, SimpleObject)]
pub struct Series {
    pub name: String,
    pub values: Vec<f64>,
    pub timestamps: Vec<String>,
}

/// Visualization configuration
#[derive(Debug, Clone, Serialize, Deserialize, SimpleObject)]
pub struct VisualizationConfig {
    pub chart_type: String,
    pub title: String,
    pub x_axis: String,
    pub y_axis: String,
}

/// Visualization result
#[derive(Debug, Clone, Serialize, Deserialize, SimpleObject)]
pub struct Visualization {
    pub chart_type: String,
    pub data: serde_json::Value,
    pub config: VisualizationConfig,
    pub insights: Vec<Insight>,
}

/// Ollama API request
#[derive(Debug, Clone, Serialize)]
pub struct OllamaRequest {
    pub model: String,
    pub prompt: String,
    pub stream: bool,
    pub options: Option<OllamaOptions>,
}

/// Ollama API options
#[derive(Debug, Clone, Serialize)]
pub struct OllamaOptions {
    pub temperature: Option<f64>,
    pub top_p: Option<f64>,
    pub top_k: Option<u32>,
    pub num_predict: Option<u32>,
}

/// Ollama API response
#[derive(Debug, Clone, Deserialize)]
pub struct OllamaResponse {
    pub model: String,
    pub created_at: String,
    pub response: String,
    pub done: bool,
    pub context: Option<Vec<u32>>,
    pub total_duration: Option<u64>,
    pub load_duration: Option<u64>,
    pub prompt_eval_count: Option<u32>,
    pub prompt_eval_duration: Option<u64>,
    pub eval_count: Option<u32>,
    pub eval_duration: Option<u64>,
}

impl Default for AgentConfig {
    fn default() -> Self {
        Self {
            agent_type: "default".to_string(),
            model: "llama2".to_string(),
            temperature: Some(0.7),
            max_tokens: Some(1000),
        }
    }
}

impl Default for OllamaOptions {
    fn default() -> Self {
        Self {
            temperature: Some(0.7),
            top_p: Some(0.9),
            top_k: Some(40),
            num_predict: Some(1000),
        }
    }
}
